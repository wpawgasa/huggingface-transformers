{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001",
   "metadata": {},
   "source": [
    "# Transformers Pipeline Exploration\n",
    "\n",
    "**Reference**: [HuggingFace LLM Course — Chapter 1.3](https://huggingface.co/learn/llm-course/chapter1/3)  \n",
    "**Design doc**: [docs/transformers_pipeline_experiment.md](../docs/transformers_pipeline_experiment.md)\n",
    "\n",
    "This notebook walks through all 10 `pipeline()` task types taught in the HuggingFace LLM Course,\n",
    "systematically benchmarking each one and exploring ablation studies.\n",
    "\n",
    "## The 3-Stage Pipeline Architecture\n",
    "\n",
    "```\n",
    "Input (text / image / audio)\n",
    "        │\n",
    "        ▼\n",
    "  Preprocessing   ← Tokenizer / Feature Extractor\n",
    "        │\n",
    "        ▼\n",
    "   Model Forward  ← AutoModel (logits)\n",
    "        │\n",
    "        ▼\n",
    " Postprocessing   ← Decode, softmax, format\n",
    "        │\n",
    "        ▼\n",
    "Output (human-readable dict)\n",
    "```\n",
    "\n",
    "## Task Inventory\n",
    "\n",
    "| # | Task | Pipeline | Architecture |\n",
    "|---|------|----------|--------------|\n",
    "| 1 | Sentiment analysis | `text-classification` | Encoder-only |\n",
    "| 2 | Zero-shot classification | `zero-shot-classification` | Encoder-decoder |\n",
    "| 3 | Text generation | `text-generation` | Decoder-only |\n",
    "| 4 | Mask filling | `fill-mask` | Encoder-only |\n",
    "| 5 | Named entity recognition | `ner` | Encoder-only |\n",
    "| 6 | Question answering | `question-answering` | Encoder-only |\n",
    "| 7 | Summarization | `summarization` | Encoder-decoder |\n",
    "| 8 | Translation | `translation` | Encoder-decoder |\n",
    "| 9 | Image classification | `image-classification` | Vision Transformer |\n",
    "| 10 | Speech recognition | `automatic-speech-recognition` | Encoder-decoder |\n",
    "\n",
    "> **Note**: First run downloads models (~10 GB total). Subsequent runs use the local cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0002",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure project root is on the path so `src` is importable\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Detect device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: pretty-print results\n",
    "def show(title, data, indent=2):\n",
    "    print(f\"\\n{'─' * 60}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'─' * 60}\")\n",
    "    print(json.dumps(data, indent=indent, default=str, ensure_ascii=False))\n",
    "\n",
    "# Collect benchmarks for the final comparison table\n",
    "benchmark_log: list[dict] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1 — Text Classification (Sentiment Analysis)\n",
    "\n",
    "**Pipeline**: `text-classification`  \n",
    "**Model**: `distilbert-base-uncased-finetuned-sst-2-english`  \n",
    "**Architecture**: Encoder-only (DistilBERT fine-tuned on SST-2)\n",
    "\n",
    "The model assigns a label (`POSITIVE` / `NEGATIVE`) and a confidence score to each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# --- Load ---\n",
    "classifier = pipeline(\"text-classification\", device=DEVICE)\n",
    "\n",
    "# --- Course examples ---\n",
    "course_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "course_outputs = classifier(course_inputs)\n",
    "show(\"Course examples\", dict(zip(course_inputs, course_outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Edge cases ---\n",
    "edge_cases = {\n",
    "    \"neutral\": \"The weather is somewhat acceptable today.\",\n",
    "    \"non_english\": \"Je suis très heureux d'être ici.\",\n",
    "    \"single_word\": \"Excellent!\",\n",
    "}\n",
    "for name, text in edge_cases.items():\n",
    "    result = classifier(text)[0]\n",
    "    print(f\"{name:15s} → {result['label']:8s} ({result['score']:.4f}) | {text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Benchmark ---\n",
    "_ = classifier(course_inputs[0])  # warm-up\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    t0 = time.perf_counter()\n",
    "    classifier(course_inputs[0])\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"text-classification\", \"model\": \"distilbert-sst2\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 10): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0020",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2 — Zero-Shot Classification\n",
    "\n",
    "**Pipeline**: `zero-shot-classification`  \n",
    "**Model**: `facebook/bart-large-mnli`  \n",
    "**Architecture**: Encoder-decoder (BART trained on MNLI)\n",
    "\n",
    "Classify text into arbitrary labels **without any task-specific fine-tuning**.\n",
    "Uses Natural Language Inference (NLI): is the text entailed by each candidate label?\n",
    "\n",
    "### Ablation: Label Count Scaling\n",
    "How does inference time scale with the number of candidate labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot = pipeline(\"zero-shot-classification\", device=DEVICE)\n",
    "\n",
    "# --- Course example ---\n",
    "sequence = \"This is a course about the Transformers library\"\n",
    "labels = [\"education\", \"politics\", \"business\"]\n",
    "result = zero_shot(sequence, candidate_labels=labels)\n",
    "show(\"Course example\", {l: round(s, 4) for l, s in zip(result[\"labels\"], result[\"scores\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ablation: label count scaling ---\n",
    "label_pool = [\"education\", \"politics\", \"business\", \"technology\", \"science\",\n",
    "              \"sports\", \"entertainment\", \"health\", \"environment\", \"culture\"]\n",
    "\n",
    "print(f\"{'n_labels':>10} | {'latency (ms)':>14} | top label\")\n",
    "print(\"-\" * 45)\n",
    "for n in [2, 3, 5, 10]:\n",
    "    sub_labels = label_pool[:n]\n",
    "    # warm-up\n",
    "    zero_shot(sequence, candidate_labels=sub_labels)\n",
    "    t0 = time.perf_counter()\n",
    "    out = zero_shot(sequence, candidate_labels=sub_labels)\n",
    "    ms = (time.perf_counter() - t0) * 1000\n",
    "    print(f\"{n:>10} | {ms:>14.1f} | {out['labels'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark (3 labels)\n",
    "_ = zero_shot(sequence, candidate_labels=labels)  # warm-up\n",
    "times = []\n",
    "for _ in range(5):\n",
    "    t0 = time.perf_counter()\n",
    "    zero_shot(sequence, candidate_labels=labels)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"zero-shot-classification\", \"model\": \"bart-large-mnli\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 5, 3 labels): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0030",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3 — Text Generation\n",
    "\n",
    "**Pipeline**: `text-generation`  \n",
    "**Default model**: `openai-community/gpt2` (GPT-2)  \n",
    "**Architecture**: Decoder-only autoregressive LM\n",
    "\n",
    "### Ablations\n",
    "1. **Temperature sweep** — {0.7, 1.0, 1.5}: lower = more focused, higher = more creative  \n",
    "2. **Model comparison** — GPT-2 vs SmolLM2-360M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\", device=DEVICE)\n",
    "\n",
    "# --- Course example ---\n",
    "prompt = \"In this course, we will teach you how to\"\n",
    "output = generator(prompt, max_new_tokens=50, num_return_sequences=1)\n",
    "print(f\"Prompt : {prompt!r}\")\n",
    "print(f\"Output : {output[0]['generated_text']!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ablation: temperature sweep ---\n",
    "print(f\"Prompt: {prompt!r}\\n\")\n",
    "for temp in [0.7, 1.0, 1.5]:\n",
    "    out = generator(prompt, max_new_tokens=40, temperature=temp,\n",
    "                    do_sample=True, num_return_sequences=1)\n",
    "    text = out[0][\"generated_text\"][len(prompt):].strip()\n",
    "    print(f\"  temp={temp} → {text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ablation: model comparison ---\n",
    "smol = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\", device=DEVICE)\n",
    "smol_out = smol(prompt, max_new_tokens=50, num_return_sequences=1)\n",
    "gpt2_out = generator(prompt, max_new_tokens=50, num_return_sequences=1)\n",
    "\n",
    "print(\"GPT-2     :\", gpt2_out[0][\"generated_text\"])\n",
    "print()\n",
    "print(\"SmolLM2   :\", smol_out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "_ = generator(prompt, max_new_tokens=50)  # warm-up\n",
    "times = []\n",
    "for _ in range(5):\n",
    "    t0 = time.perf_counter()\n",
    "    generator(prompt, max_new_tokens=50)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"text-generation\", \"model\": \"gpt2\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 5, 50 new tokens): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0040",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4 — Fill-Mask\n",
    "\n",
    "**Pipeline**: `fill-mask`  \n",
    "**Default model**: `distilroberta-base` (uses `<mask>` token)  \n",
    "**Architecture**: Encoder-only MLM\n",
    "\n",
    "### Ablation: mask token / model\n",
    "- DistilRoBERTa uses `<mask>` \n",
    "- BERT-base-cased uses `[MASK]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(\"fill-mask\", device=DEVICE)  # default: distilroberta-base\n",
    "\n",
    "# --- Course example ---\n",
    "text = \"This course will teach you all about <mask> models.\"\n",
    "results = unmasker(text)\n",
    "print(f\"Input: {text!r}\")\n",
    "print(\"\\nTop predictions:\")\n",
    "for r in results[:5]:\n",
    "    print(f\"  {r['token_str']:15s} → score={r['score']:.4f} | {r['sequence']!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ablation: BERT-base-cased with [MASK] token ---\n",
    "bert_unmasker = pipeline(\"fill-mask\", model=\"bert-base-cased\", device=DEVICE)\n",
    "bert_text = \"This course will teach you all about [MASK] models.\"\n",
    "bert_results = bert_unmasker(bert_text)\n",
    "\n",
    "print(\"DistilRoBERTa (<mask>):\")\n",
    "for r in results[:3]:\n",
    "    print(f\"  {r['token_str']:15s} score={r['score']:.4f}\")\n",
    "\n",
    "print(\"\\nBERT-base-cased ([MASK]):\")\n",
    "for r in bert_results[:3]:\n",
    "    print(f\"  {r['token_str']:15s} score={r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "_ = unmasker(text)  # warm-up\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    t0 = time.perf_counter()\n",
    "    unmasker(text)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"fill-mask\", \"model\": \"distilroberta-base\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 10): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0050",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 5 — Named Entity Recognition (NER)\n",
    "\n",
    "**Pipeline**: `ner`  \n",
    "**Model**: `dbmdz/bert-large-cased-finetuned-conll03-english`  \n",
    "**Architecture**: Encoder-only (BERT-large with token classification head)\n",
    "\n",
    "### Ablation: `grouped_entities`\n",
    "- `True` → merge subword tokens into whole words/spans  \n",
    "- `False` → raw per-token predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True, device=DEVICE)\n",
    "\n",
    "# --- Course example ---\n",
    "text = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "entities = ner(text)\n",
    "show(\"Course example (grouped_entities=True)\", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ablation: grouped vs ungrouped ---\n",
    "ner_raw = pipeline(\"ner\", grouped_entities=False, device=DEVICE)\n",
    "raw_entities = ner_raw(text)\n",
    "\n",
    "print(f\"grouped=True  → {len(entities)} entities\")\n",
    "for e in entities:\n",
    "    print(f\"  [{e['entity_group']:3s}] {e['word']!r:20s} ({e['score']:.4f})\")\n",
    "\n",
    "print(f\"\\ngrouped=False → {len(raw_entities)} tokens\")\n",
    "for e in raw_entities:\n",
    "    print(f\"  [{e['entity']:6s}] {e['word']!r:15s} ({e['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "_ = ner(text)  # warm-up\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    t0 = time.perf_counter()\n",
    "    ner(text)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"ner\", \"model\": \"bert-large-conll03\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 10): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0060",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 6 — Question Answering (Extractive QA)\n",
    "\n",
    "**Pipeline**: `question-answering`  \n",
    "**Model**: `distilbert-base-cased-distilled-squad`  \n",
    "**Architecture**: Encoder-only (DistilBERT with span-extraction head)\n",
    "\n",
    "Extracts the answer span directly from the provided context — no generation involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pipeline(\"question-answering\", device=DEVICE)\n",
    "\n",
    "# --- Course example ---\n",
    "result = qa(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")\n",
    "show(\"Course example\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Edge cases ---\n",
    "qa_pairs = [\n",
    "    {\n",
    "        \"name\": \"multi-sentence context\",\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"context\": \"France is a country in Western Europe. Its capital city is Paris. The Eiffel Tower is located there.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"technical question\",\n",
    "        \"question\": \"What does the pipeline() function do?\",\n",
    "        \"context\": \"The pipeline() function in HuggingFace Transformers is the highest-level entry point for NLP tasks. It encapsulates preprocessing, model forward pass, and postprocessing in a single callable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for item in qa_pairs:\n",
    "    res = qa(question=item[\"question\"], context=item[\"context\"])\n",
    "    print(f\"[{item['name']}]\")\n",
    "    print(f\"  Q: {item['question']}\")\n",
    "    print(f\"  A: {res['answer']!r}  (score={res['score']:.4f}, span=[{res['start']}:{res['end']}])\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "bench_input = {\"question\": \"Where do I work?\",\n",
    "               \"context\": \"My name is Sylvain and I work at Hugging Face in Brooklyn\"}\n",
    "_ = qa(**bench_input)  # warm-up\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    t0 = time.perf_counter()\n",
    "    qa(**bench_input)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"question-answering\", \"model\": \"distilbert-squad\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 10): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0070",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 7 — Summarization\n",
    "\n",
    "**Pipeline**: `summarization`  \n",
    "**Model**: `sshleifer/distilbart-cnn-12-6` (DistilBART-CNN)  \n",
    "**Architecture**: Encoder-decoder (BART distilled on CNN/DM)\n",
    "\n",
    "### Ablation: max_length\n",
    "How does the summary quality change as we allow more output tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", device=DEVICE)\n",
    "\n",
    "# --- Course example ---\n",
    "article = (\n",
    "    \"America has changed dramatically during recent years. Not only has the number of \"\n",
    "    \"graduates in traditional engineering disciplines such as mechanical, civil, \"\n",
    "    \"electrical, chemical, and aeronautical engineering declined, but in most of \"\n",
    "    \"the premier universities in the United States, many of the valedictorians \"\n",
    "    \"and high-ranking students are girls. Increasingly, these students are \"\n",
    "    \"particularly in programs such as health and medical related fields, \"\n",
    "    \"including premedical programs, medicine, law, business, and computer \"\n",
    "    \"sciences. For women, the attractiveness of these majors is that they \"\n",
    "    \"offer the opportunity to combine intellectual rigor and career potential \"\n",
    "    \"with an altruistic contribution to society. This particular article, \"\n",
    "    \"however, is about another major -- the STEM fields, which stands for \"\n",
    "    \"science, technology, engineering, and mathematics.\"\n",
    ")\n",
    "\n",
    "result = summarizer(article, max_length=130, min_length=30)\n",
    "print(\"Summary:\")\n",
    "print(result[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ablation: max_length sweep ---\n",
    "for max_len in [50, 100, 200]:\n",
    "    out = summarizer(article, max_length=max_len, min_length=min(20, max_len - 10))\n",
    "    summary = out[0][\"summary_text\"]\n",
    "    print(f\"max_length={max_len:3d} → {len(summary.split()):3d} words: {summary[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "_ = summarizer(article, max_length=130, min_length=30)  # warm-up\n",
    "times = []\n",
    "for _ in range(3):\n",
    "    t0 = time.perf_counter()\n",
    "    summarizer(article, max_length=130, min_length=30)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"summarization\", \"model\": \"distilbart-cnn-12-6\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 3): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0080",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 8 — Translation (French → English)\n",
    "\n",
    "**Pipeline**: `translation`  \n",
    "**Model**: `Helsinki-NLP/opus-mt-fr-en` (MarianMT)  \n",
    "**Architecture**: Encoder-decoder (seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\", device=DEVICE)\n",
    "\n",
    "# --- Course example ---\n",
    "fr_text = \"Ce cours est produit par Hugging Face.\"\n",
    "result = translator(fr_text)\n",
    "print(f\"FR: {fr_text}\")\n",
    "print(f\"EN: {result[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Edge cases ---\n",
    "sentences = [\n",
    "    \"Bonjour, comment allez-vous?\",\n",
    "    \"Qu'est-ce que l'intelligence artificielle?\",\n",
    "    \"Les modèles de transformateur révolutionnent le traitement du langage naturel.\",\n",
    "]\n",
    "for fr in sentences:\n",
    "    en = translator(fr)[0][\"translation_text\"]\n",
    "    print(f\"FR: {fr}\")\n",
    "    print(f\"EN: {en}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "_ = translator(fr_text)  # warm-up\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    t0 = time.perf_counter()\n",
    "    translator(fr_text)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"translation\", \"model\": \"opus-mt-fr-en\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 10): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0090",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 9 — Image Classification\n",
    "\n",
    "**Pipeline**: `image-classification`  \n",
    "**Model**: `google/vit-base-patch16-224` (ViT-Base)  \n",
    "**Architecture**: Vision Transformer (patches the image into 16×16 tokens)\n",
    "\n",
    "The pipeline downloads the image, preprocesses it into 224×224 patches, and\n",
    "returns top-k class probabilities from ImageNet-21k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "img_classifier = pipeline(\"image-classification\", device=DEVICE)\n",
    "\n",
    "# --- Course example ---\n",
    "cat_url = (\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images\"\n",
    "    \"/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "display(IPyImage(url=cat_url, width=300))\n",
    "\n",
    "results = img_classifier(cat_url)\n",
    "print(\"\\nTop-5 predictions:\")\n",
    "for r in results[:5]:\n",
    "    print(f\"  {r['label']:40s} {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "_ = img_classifier(cat_url)  # warm-up\n",
    "times = []\n",
    "for _ in range(5):\n",
    "    t0 = time.perf_counter()\n",
    "    img_classifier(cat_url)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"image-classification\", \"model\": \"vit-base-patch16-224\",\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 5): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0100",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 10 — Automatic Speech Recognition (ASR)\n",
    "\n",
    "**Pipeline**: `automatic-speech-recognition`  \n",
    "**Model (CPU)**: `openai/whisper-tiny` — ~150 MB, fast on CPU  \n",
    "**Model (GPU)**: `openai/whisper-large-v3` — ~3 GB, high accuracy  \n",
    "**Architecture**: Encoder-decoder (Whisper)\n",
    "\n",
    "### Ablation: CPU model substitution\n",
    "whisper-tiny (CPU) vs whisper-large-v3 (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-select model based on device\n",
    "asr_model = \"openai/whisper-large-v3\" if DEVICE == \"cuda\" else \"openai/whisper-tiny\"\n",
    "print(f\"Using ASR model: {asr_model} (device={DEVICE})\")\n",
    "\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=asr_model, device=DEVICE)\n",
    "\n",
    "# --- Course example: MLK speech excerpt ---\n",
    "audio_url = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
    "result = transcriber(audio_url)\n",
    "print(f\"\\nTranscription: {result['text']!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case: shorter audio\n",
    "short_url = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\"\n",
    "short_result = transcriber(short_url)\n",
    "print(f\"Short clip: {short_result['text']!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "_ = transcriber(audio_url)  # warm-up\n",
    "times = []\n",
    "for _ in range(3):\n",
    "    t0 = time.perf_counter()\n",
    "    transcriber(audio_url)\n",
    "    times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "bm = {\"task\": \"automatic-speech-recognition\", \"model\": asr_model.split(\"/\")[-1],\n",
    "      \"warm_latency_ms\": round(sum(times) / len(times), 1)}\n",
    "benchmark_log.append(bm)\n",
    "print(f\"Warm latency (mean of 3): {bm['warm_latency_ms']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0110",
   "metadata": {},
   "source": [
    "---\n",
    "## Results: Benchmark Comparison\n",
    "\n",
    "Warm inference latency across all 10 tasks (single input, CPU unless GPU detected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(benchmark_log)\n",
    "df = df.sort_values(\"warm_latency_ms\")\n",
    "df.index = range(1, len(df) + 1)\n",
    "print(f\"Device: {DEVICE}\\n\")\n",
    "print(df.to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = [\"#4CAF50\" if ms < 200 else \"#FF9800\" if ms < 1000 else \"#F44336\"\n",
    "          for ms in df[\"warm_latency_ms\"]]\n",
    "bars = ax.barh(df[\"task\"], df[\"warm_latency_ms\"], color=colors)\n",
    "ax.bar_label(bars, fmt=\"%.0f ms\", padding=4)\n",
    "ax.set_xlabel(\"Warm latency (ms)\")\n",
    "ax.set_title(f\"Pipeline Latency Comparison ({DEVICE})\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/benchmark_chart.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Chart saved to results/benchmark_chart.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results\n",
    "import os\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "df.to_csv(\"../results/notebook_benchmarks.csv\", index=False)\n",
    "print(\"Saved to results/notebook_benchmarks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0120",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Encoder-only models** (DistilBERT, DistilRoBERTa) are fastest (~50–150 ms) — ideal for classification and extraction.\n",
    "\n",
    "2. **Encoder-decoder models** (BART, MarianMT, DistilBART) are slower (~200–2000 ms) — required for generation and seq2seq tasks.\n",
    "\n",
    "3. **Vision Transformers** (ViT) have comparable latency to text encoders — the image is simply tokenized as patches.\n",
    "\n",
    "4. **ASR** (Whisper) is the slowest on CPU — audio requires far more computation than text.\n",
    "\n",
    "5. **Zero-shot classification** scales linearly with label count — each label adds one NLI pass.\n",
    "\n",
    "6. **Temperature** affects diversity but not speed in text generation — all temperatures have the same forward-pass cost.\n",
    "\n",
    "7. **The `pipeline()` abstraction** seamlessly handles:\n",
    "   - Different input types (string, dict, URL, numpy array)\n",
    "   - Different output schemas per task\n",
    "   - Device placement (CPU / GPU)\n",
    "   - Batching and tokenization\n",
    "\n",
    "---\n",
    "*Reference*: [HuggingFace LLM Course — Chapter 1.3](https://huggingface.co/learn/llm-course/chapter1/3)"
   ]
  }
 ]
}
