{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture Deep Dive\n",
    "\n",
    "**Reference**: [HuggingFace LLM Course — Chapter 1.4](https://huggingface.co/learn/llm-course/chapter1/4)  \n",
    "**Design doc**: [docs/transformer_architecture_experiment.md](../docs/transformer_architecture_experiment.md)\n",
    "\n",
    "This notebook walks through 6 probes that empirically verify the theoretical claims from\n",
    "Chapter 1.4 about attention mechanisms, architecture families, language modeling objectives,\n",
    "and transfer learning.\n",
    "\n",
    "## The Three Architecture Families\n",
    "\n",
    "```\n",
    "┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────────────┐\n",
    "│  Encoder-Only       │  │  Decoder-Only        │  │  Encoder-Decoder            │\n",
    "│  (BERT-like)        │  │  (GPT-like)          │  │  (T5-like)                  │\n",
    "│                     │  │                      │  │                             │\n",
    "│  Bidirectional      │  │  Causal (L→R)        │  │  Bidir enc + causal dec     │\n",
    "│  attention          │  │  attention            │  │  + cross-attention          │\n",
    "│                     │  │                      │  │                             │\n",
    "│  → Classification   │  │  → Text generation   │  │  → Translation              │\n",
    "│  → NER, QA          │  │  → Code completion   │  │  → Summarization            │\n",
    "└─────────────────────┘  └─────────────────────┘  └─────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Probe Overview\n",
    "\n",
    "| # | Probe | What We Measure |\n",
    "|---|-------|-----------------|\n",
    "| 1 | Transformer Timeline | Family distribution, scale trends |\n",
    "| 2 | Causal vs Masked LM | Token predictions, context directionality |\n",
    "| 3 | Transfer Learning | Pretrained vs scratch accuracy gap |\n",
    "| 4 | Model Anatomy | Parameter counts, layer breakdown |\n",
    "| 5 | Attention Visualization | Attention matrices, coreference resolution |\n",
    "| 6 | Architecture Comparison | Hidden states, masking patterns, output formats |\n",
    "\n",
    "> **Note**: Probes 2–6 download models on first run (~2 GB total). Subsequent runs use the local cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env from project root\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "# Ensure project root is on the path so `src.architecture_deepdive` is importable\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Detect device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(\"../results/figures\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: pretty-print results\n",
    "def show(title, data, indent=2):\n",
    "    print(f\"\\n{'─' * 60}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'─' * 60}\")\n",
    "    print(json.dumps(data, indent=indent, default=str, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Probe 1 — Transformer History Timeline\n",
    "\n",
    "**Module**: `p1_model_timeline.py`  \n",
    "**Models used**: None (data only)  \n",
    "\n",
    "From the course: *\"A bit of Transformer history\"*\n",
    "\n",
    "The original Transformer was introduced in June 2017. Since then, models have been\n",
    "organized into three families:\n",
    "- **GPT-like** (auto-regressive / decoder-only)\n",
    "- **BERT-like** (auto-encoding / encoder-only)\n",
    "- **T5-like** (sequence-to-sequence / encoder-decoder)\n",
    "\n",
    "This probe builds a structured timeline and analyzes the family distribution over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architecture_deepdive.probes.p1_model_timeline import TIMELINE, run_experiment\n",
    "\n",
    "result_p1 = run_experiment()\n",
    "\n",
    "# Display the timeline\n",
    "print(f\"{'Model':<25} {'Date':<18} {'Params':<12} {'Family'}\")\n",
    "print(\"─\" * 75)\n",
    "for m in TIMELINE:\n",
    "    print(f\"{m.name:<25} {m.date:<18} {m.params:<12} {m.family}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family distribution\n",
    "print(\"\\nFamily Distribution:\")\n",
    "for family, count in result_p1[\"family_distribution\"].items():\n",
    "    bar = \"█\" * (count * 3)\n",
    "    print(f\"  {family:<18} {bar} {count}/{len(TIMELINE)}\")\n",
    "\n",
    "print(f\"\\nObservation: {result_p1['observation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datetime import datetime\n\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\n\n# Visualize: family distribution pie chart + timeline\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Pie chart\ndist = result_p1[\"family_distribution\"]\ncolors = {\"decoder-only\": \"#2196F3\", \"encoder-only\": \"#4CAF50\", \"encoder-decoder\": \"#FF9800\"}\nax1.pie(\n    dist.values(),\n    labels=[f\"{k}\\n({v})\" for k, v in dist.items()],\n    colors=[colors.get(k, \"#999\") for k in dist],\n    autopct=\"%1.0f%%\",\n    startangle=90,\n    textprops={\"fontsize\": 10},\n)\nax1.set_title(\"Architecture Family Distribution\\n(Models from Course Timeline)\")\n\n# Timeline scatter\nfor m in TIMELINE:\n    try:\n        date = datetime.strptime(m.date, \"%B %Y\")\n    except ValueError:\n        continue\n    color = colors.get(m.family, \"#999\")\n    ax2.scatter(date, m.family, color=color, s=100, zorder=3)\n    ax2.annotate(\n        m.name,\n        (date, m.family),\n        textcoords=\"offset points\",\n        xytext=(5, 8),\n        fontsize=7,\n        rotation=30,\n    )\n\nax2.set_title(\"Transformer Model Timeline\")\nax2.set_xlabel(\"Date\")\nax2.grid(True, alpha=0.3)\nax2.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n\nplt.tight_layout()\nplt.savefig(\"../results/figures/p1_timeline.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(\"Saved: results/figures/p1_timeline.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Probe 2 — Causal vs Masked Language Modeling\n",
    "\n",
    "**Module**: `p2_language_modeling.py`  \n",
    "**Models**: GPT-2 (Causal LM) and BERT-base (Masked LM)  \n",
    "\n",
    "From the course:\n",
    "> *\"Causal language modeling: predicting the next word having read the n previous words\"*  \n",
    "> *\"Masked language modeling: the model predicts a masked word in the sentence\"*\n",
    "\n",
    "### Key Investigation\n",
    "How does context directionality affect predictions?  \n",
    "- **CLM (GPT)**: Only sees tokens to the LEFT of the current position\n",
    "- **MLM (BERT)**: Sees tokens to the LEFT and RIGHT of the masked position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architecture_deepdive.probes.p2_language_modeling import (\n",
    "    run_causal_lm_probe,\n",
    "    run_masked_lm_probe,\n",
    ")\n",
    "\n",
    "# --- 2.1: Causal LM (GPT-2) ---\n",
    "print(\"═\" * 60)\n",
    "print(\"  CAUSAL LANGUAGE MODELING (GPT-2)\")\n",
    "print(\"  Context: left-to-right only\")\n",
    "print(\"═\" * 60)\n",
    "\n",
    "clm_result = run_causal_lm_probe(device=DEVICE)\n",
    "\n",
    "for r in clm_result[\"results\"]:\n",
    "    print(f\"\\nPrompt: {r['prompt']!r}\")\n",
    "    print(f\"  Direction: {r['context_direction']}\")\n",
    "    print(\"  Top 5 next-token predictions:\")\n",
    "    for p in r[\"top_5_predictions\"]:\n",
    "        bar = \"█\" * int(p[\"probability\"] * 40)\n",
    "        print(f\"    {p['token']:15s} {bar} {p['probability']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.2: Masked LM (BERT) ---\n",
    "print(\"═\" * 60)\n",
    "print(\"  MASKED LANGUAGE MODELING (BERT)\")\n",
    "print(\"  Context: bidirectional (sees LEFT and RIGHT)\")\n",
    "print(\"═\" * 60)\n",
    "\n",
    "mlm_result = run_masked_lm_probe(device=DEVICE)\n",
    "\n",
    "for r in mlm_result[\"results\"]:\n",
    "    print(f\"\\nSentence: {r['sentence']!r}\")\n",
    "    print(f\"  Direction: {r['context_direction']}\")\n",
    "    print(\"  Top 5 predictions for [MASK]:\")\n",
    "    for p in r[\"top_5_predictions\"]:\n",
    "        bar = \"█\" * int(p[\"probability\"] * 40)\n",
    "        print(f\"    {p['token']:15s} {bar} {p['probability']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.3: Comparison ---\n",
    "print(\"\\n\" + \"═\" * 60)\n",
    "print(\"  KEY COMPARISON: CLM vs MLM\")\n",
    "print(\"═\" * 60)\n",
    "print(\"\"\"\n",
    "Shared test: Predicting the word after 'The capital of France is'\n",
    "\n",
    "  CLM (GPT-2):\n",
    "    Sees: 'The capital of France is' → predicts next token\n",
    "    Can only use LEFT context (past tokens)\n",
    "\n",
    "  MLM (BERT):\n",
    "    Sees: 'The capital of France is [MASK] .' → predicts masked token\n",
    "    Uses BOTH left and right context (including the period)\n",
    "\n",
    "  Key difference:\n",
    "    This is why BERT is better for understanding tasks (classification, NER)\n",
    "    and GPT is better for generation tasks (text completion, chat).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Probe 3 — Transfer Learning: Pretrained vs From-Scratch\n",
    "\n",
    "**Module**: `p3_transfer_learning.py`  \n",
    "**Model**: Tiny BERT (`google/bert_uncased_L-2_H-128_A-2`)  \n",
    "\n",
    "From the course:\n",
    "> *\"The pretrained model was already trained on a dataset that has some similarities\n",
    "> with the fine-tuning dataset. The fine-tuning process is thus able to take advantage\n",
    "> of knowledge acquired during pretraining.\"*\n",
    "\n",
    "### Experiment Design\n",
    "- **Dataset**: 8 training + 4 test sentiment examples (tiny!)\n",
    "- **Pretrained**: Fine-tune a pretrained BERT on this data\n",
    "- **From-scratch**: Train a randomly initialized BERT on the same data\n",
    "- **Hypothesis**: Pretrained should converge faster and achieve higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architecture_deepdive.data import TRANSFER_LEARNING_DATA\n",
    "\n",
    "# Show the dataset\n",
    "print(\"Training Data (8 examples):\")\n",
    "for text, label in TRANSFER_LEARNING_DATA[\"train\"]:\n",
    "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
    "    print(f\"  [{sentiment:>8}] {text}\")\n",
    "\n",
    "print(\"\\nTest Data (4 examples):\")\n",
    "for text, label in TRANSFER_LEARNING_DATA[\"test\"]:\n",
    "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
    "    print(f\"  [{sentiment:>8}] {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architecture_deepdive.probes.p3_transfer_learning import train_and_evaluate\n",
    "\n",
    "model_name = \"google/bert_uncased_L-2_H-128_A-2\"  # Tiny BERT\n",
    "num_epochs = 10\n",
    "\n",
    "# --- 3.1: Pretrained fine-tuning ---\n",
    "print(\"Training pretrained model...\")\n",
    "t0 = time.perf_counter()\n",
    "pretrained_result = train_and_evaluate(\n",
    "    model_name, from_scratch=False, num_epochs=num_epochs, device=DEVICE\n",
    ")\n",
    "pt_time = time.perf_counter() - t0\n",
    "print(f\"  Done in {pt_time:.1f}s — Final accuracy: {pretrained_result['final_accuracy']:.2%}\")\n",
    "\n",
    "# --- 3.2: From-scratch training ---\n",
    "print(\"\\nTraining from-scratch model...\")\n",
    "t0 = time.perf_counter()\n",
    "scratch_result = train_and_evaluate(\n",
    "    model_name, from_scratch=True, num_epochs=num_epochs, device=DEVICE\n",
    ")\n",
    "sc_time = time.perf_counter() - t0\n",
    "print(f\"  Done in {sc_time:.1f}s — Final accuracy: {scratch_result['final_accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.3: Epoch-by-epoch comparison table ---\n",
    "pt_hist = pretrained_result[\"history\"]\n",
    "sc_hist = scratch_result[\"history\"]\n",
    "\n",
    "print(\n",
    "    f\"{'Epoch':>6} │ {'Pretrained Acc':>15} {'Pretrained Loss':>16} │ {'Scratch Acc':>12} {'Scratch Loss':>14}\"\n",
    ")\n",
    "print(\"─\" * 72)\n",
    "for i in range(num_epochs):\n",
    "    print(\n",
    "        f\"{pt_hist['epochs'][i]:>6} │ \"\n",
    "        f\"{pt_hist['test_accuracy'][i]:>14.2%} {pt_hist['train_loss'][i]:>15.4f} │ \"\n",
    "        f\"{sc_hist['test_accuracy'][i]:>11.2%} {sc_hist['train_loss'][i]:>13.4f}\"\n",
    "    )\n",
    "\n",
    "gap = pretrained_result[\"final_accuracy\"] - scratch_result[\"final_accuracy\"]\n",
    "print(f\"\\nAccuracy gap: {gap:+.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4: Learning curves plot ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = pt_hist[\"epochs\"]\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(\n",
    "    epochs,\n",
    "    pt_hist[\"test_accuracy\"],\n",
    "    \"o-\",\n",
    "    color=\"#4CAF50\",\n",
    "    label=\"Pretrained + fine-tuned\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax1.plot(\n",
    "    epochs, sc_hist[\"test_accuracy\"], \"s--\", color=\"#F44336\", label=\"From scratch\", linewidth=2\n",
    ")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Test Accuracy\")\n",
    "ax1.set_title(\"Transfer Learning: Accuracy Comparison\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(\n",
    "    epochs,\n",
    "    pt_hist[\"train_loss\"],\n",
    "    \"o-\",\n",
    "    color=\"#4CAF50\",\n",
    "    label=\"Pretrained + fine-tuned\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax2.plot(epochs, sc_hist[\"train_loss\"], \"s--\", color=\"#F44336\", label=\"From scratch\", linewidth=2)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Training Loss\")\n",
    "ax2.set_title(\"Transfer Learning: Loss Comparison\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Probe 3: Transfer Learning — Pretrained vs From-Scratch\\n\"\n",
    "    f\"(Model: {model_name}, {num_epochs} epochs, dataset: 8 train / 4 test)\",\n",
    "    fontsize=12,\n",
    "    y=1.03,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/figures/p3_transfer_learning.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: results/figures/p3_transfer_learning.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Probe 4 — Model Anatomy: Architecture vs Checkpoint\n",
    "\n",
    "**Module**: `p4_model_anatomy.py`  \n",
    "**Models**: BERT-base, GPT-2, T5-small  \n",
    "\n",
    "From the course:\n",
    "> *\"Architecture: the skeleton of the model — the definition of each layer\n",
    "> and each operation that happens within the model.\"*  \n",
    "> *\"Checkpoints: the weights that will be loaded in a given architecture.\"*\n",
    "\n",
    "We inspect one representative model from each family to understand their internal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architecture_deepdive.data import MODEL_REGISTRY\n",
    "from src.architecture_deepdive.utils.model_inspector import format_param_count, inspect_model\n",
    "\n",
    "anatomies = {}\n",
    "for family_key, info in MODEL_REGISTRY.items():\n",
    "    model_name = info[\"primary\"]\n",
    "    print(f\"Inspecting {model_name} ({family_key})...\")\n",
    "    anatomy = inspect_model(model_name, family_key)\n",
    "    anatomies[family_key] = anatomy\n",
    "    print(\n",
    "        f\"  → {format_param_count(anatomy.num_parameters)} parameters, \"\n",
    "        f\"{anatomy.num_layers} layers, hidden={anatomy.hidden_size}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1: Side-by-side comparison table ---\n",
    "print(f\"{'Property':<30} {'BERT-base':>15} {'GPT-2':>15} {'T5-small':>15}\")\n",
    "print(\"─\" * 78)\n",
    "\n",
    "enc = anatomies[\"encoder_only\"]\n",
    "dec = anatomies[\"decoder_only\"]\n",
    "encdec = anatomies[\"encoder_decoder\"]\n",
    "\n",
    "rows = [\n",
    "    (\n",
    "        \"Architecture class\",\n",
    "        enc.architecture_class,\n",
    "        dec.architecture_class,\n",
    "        encdec.architecture_class,\n",
    "    ),\n",
    "    (\n",
    "        \"Parameters\",\n",
    "        format_param_count(enc.num_parameters),\n",
    "        format_param_count(dec.num_parameters),\n",
    "        format_param_count(encdec.num_parameters),\n",
    "    ),\n",
    "    (\"Layers\", str(enc.num_layers), str(dec.num_layers), str(encdec.num_layers)),\n",
    "    (\"Hidden size\", str(enc.hidden_size), str(dec.hidden_size), str(encdec.hidden_size)),\n",
    "    (\n",
    "        \"Attention heads\",\n",
    "        str(enc.num_attention_heads),\n",
    "        str(dec.num_attention_heads),\n",
    "        str(encdec.num_attention_heads),\n",
    "    ),\n",
    "    (\"Vocab size\", f\"{enc.vocab_size:,}\", f\"{dec.vocab_size:,}\", f\"{encdec.vocab_size:,}\"),\n",
    "    (\n",
    "        \"Max positions\",\n",
    "        f\"{enc.max_position_embeddings:,}\",\n",
    "        f\"{dec.max_position_embeddings:,}\",\n",
    "        f\"{encdec.max_position_embeddings:,}\",\n",
    "    ),\n",
    "    (\"Has encoder\", str(enc.has_encoder), str(dec.has_encoder), str(encdec.has_encoder)),\n",
    "    (\"Has decoder\", str(enc.has_decoder), str(dec.has_decoder), str(encdec.has_decoder)),\n",
    "]\n",
    "\n",
    "for prop, v1, v2, v3 in rows:\n",
    "    print(f\"{prop:<30} {v1:>15} {v2:>15} {v3:>15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2: Layer breakdown for each model ---\n",
    "for family_key, anatomy in anatomies.items():\n",
    "    info = MODEL_REGISTRY[family_key]\n",
    "    print(f\"\\n{'═' * 50}\")\n",
    "    print(f\"  {info['primary']} ({info['family']})\")\n",
    "    print(f\"  Objective: {info['objective']}\")\n",
    "    print(f\"  Attention: {info['attention']}\")\n",
    "    print(f\"{'═' * 50}\")\n",
    "    print(f\"{'Module':<30} {'Tensors':>10} {'Parameters':>15}\")\n",
    "    print(\"─\" * 58)\n",
    "    for module, breakdown in anatomy.layer_breakdown.items():\n",
    "        print(\n",
    "            f\"{module:<30} {breakdown['count']:>10} {format_param_count(breakdown['params']):>15}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from matplotlib.patches import Patch\n\n# --- 4.3: Parameter comparison chart ---\nmodel_data = [\n    {\"name\": a.name, \"family\": fk, \"params\": a.num_parameters} for fk, a in anatomies.items()\n]\n\ncolors = {\n    \"encoder_only\": \"#4CAF50\",\n    \"decoder_only\": \"#2196F3\",\n    \"encoder_decoder\": \"#FF9800\",\n}\nnames = [d[\"name\"] for d in model_data]\nparams_m = [d[\"params\"] / 1e6 for d in model_data]\nbar_colors = [colors[d[\"family\"]] for d in model_data]\n\nfig, ax = plt.subplots(figsize=(10, 4))\nbars = ax.barh(names, params_m, color=bar_colors, edgecolor=\"white\")\nax.bar_label(bars, fmt=\"%.1fM\", padding=4)\nax.set_xlabel(\"Parameters (Millions)\")\nax.set_title(\"Parameter Count by Architecture Family\")\n\nlegend_elements = [\n    Patch(facecolor=\"#4CAF50\", label=\"Encoder-only\"),\n    Patch(facecolor=\"#2196F3\", label=\"Decoder-only\"),\n    Patch(facecolor=\"#FF9800\", label=\"Encoder-decoder\"),\n]\nax.legend(handles=legend_elements, loc=\"lower right\")\nplt.tight_layout()\nplt.savefig(\"../results/figures/p4_parameter_comparison.png\", dpi=150)\nplt.show()\nprint(\"Saved: results/figures/p4_parameter_comparison.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Probe 5 — Attention Layer Visualization\n",
    "\n",
    "**Module**: `p5_attention_viz.py`  \n",
    "**Models**: BERT-base (bidirectional) and GPT-2 (causal)  \n",
    "\n",
    "From the course:\n",
    "> *\"This layer will tell the model to pay specific attention to certain words\n",
    "> in the sentence you passed it (and more or less ignore the others).\"*\n",
    "\n",
    "> *\"A translation model will need to also attend to the adjacent word 'You'\n",
    "> to get the proper translation for the word 'like'.\"*\n",
    "\n",
    "### What We'll Verify\n",
    "1. **Bidirectional vs Causal masks** — theoretical comparison\n",
    "2. **BERT attention** — full matrices (every token sees every token)\n",
    "3. **Coreference resolution** — does \"it\" attend to \"animal\" vs \"street\"?\n",
    "4. **GPT-2 attention** — lower-triangular causal pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport seaborn as sns\n\nfrom src.architecture_deepdive.data import ATTENTION_SENTENCES\nfrom src.architecture_deepdive.utils.attention_tools import (\n    compare_causal_vs_bidirectional_mask,\n    extract_attention_weights,\n    get_attention_to_token,\n)\n\n# --- 5.1: Attention mask comparison (theoretical) ---\ntokens_demo = [\"You\", \"like\", \"this\", \"course\"]\nmasks = compare_causal_vs_bidirectional_mask(seq_len=len(tokens_demo))\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n\nfor ax, mask, title in zip(\n    axes,\n    [masks[\"bidirectional\"], masks[\"causal\"]],\n    [\"Bidirectional (Encoder / BERT)\", \"Causal (Decoder / GPT)\"],\n    strict=True,\n):\n    sns.heatmap(\n        mask,\n        xticklabels=tokens_demo,\n        yticklabels=tokens_demo,\n        cmap=\"Blues\",\n        vmin=0,\n        vmax=1,\n        cbar=False,\n        ax=ax,\n        linewidths=0.5,\n        linecolor=\"gray\",\n        annot=True,\n        fmt=\".0f\",\n    )\n    ax.set_title(title, fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Key position\")\n    ax.set_ylabel(\"Query position\")\n\nplt.suptitle(\n    \"Attention Mask Patterns — The Key Architectural Difference\",\n    fontsize=13,\n    fontweight=\"bold\",\n    y=1.02,\n)\nplt.tight_layout()\nplt.savefig(\"../results/figures/p5_mask_comparison.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\nprint(f\"\\n{masks['note']}\")\nprint(f\"Masked positions in causal model: {masks['causal_masked_positions']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- 5.2: BERT attention on key sentences ---\nbert_model = \"bert-base-uncased\"\n\n# Extract attention for coreference sentences\nsentences_to_plot = [\n    (\"coref_animal\", ATTENTION_SENTENCES[\"coref_animal\"]),\n    (\"coref_street\", ATTENTION_SENTENCES[\"coref_street\"]),\n]\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor ax, (key, sentence) in zip(axes, sentences_to_plot, strict=True):\n    attn_data = extract_attention_weights(bert_model, sentence, DEVICE)\n    # Last layer, first head (more task-specific patterns)\n    attn = attn_data[\"attentions\"][-1, 0]  # (seq, seq)\n    tokens = attn_data[\"tokens\"]\n\n    sns.heatmap(\n        attn,\n        xticklabels=tokens,\n        yticklabels=tokens,\n        cmap=\"YlOrRd\",\n        vmin=0,\n        vmax=None,\n        ax=ax,\n        annot=len(tokens) <= 15,\n        fmt=\".2f\",\n    )\n    ax.set_title(f\"BERT (Last Layer): {key}\\n{sentence}\", fontsize=9)\n    ax.set_xlabel(\"Key\")\n    ax.set_ylabel(\"Query\")\n    ax.tick_params(axis=\"x\", rotation=45)\n    ax.tick_params(axis=\"y\", rotation=0)\n\nplt.suptitle(\n    \"BERT Bidirectional Attention — Coreference Resolution\", fontsize=13, fontweight=\"bold\"\n)\nplt.tight_layout()\nplt.savefig(\"../results/figures/p5_bert_coreference.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.3: Coreference test — \"it\" → \"animal\" vs \"it\" → \"street\" ---\n",
    "coref_tired = extract_attention_weights(bert_model, ATTENTION_SENTENCES[\"coref_animal\"], DEVICE)\n",
    "coref_wide = extract_attention_weights(bert_model, ATTENTION_SENTENCES[\"coref_street\"], DEVICE)\n",
    "\n",
    "it_to_animal = get_attention_to_token(\n",
    "    coref_tired[\"attentions\"], coref_tired[\"tokens\"], \"animal\", layer=-1\n",
    ")\n",
    "it_to_street = get_attention_to_token(\n",
    "    coref_wide[\"attentions\"], coref_wide[\"tokens\"], \"street\", layer=-1\n",
    ")\n",
    "\n",
    "print(\"Coreference Resolution Test\")\n",
    "print(\"═\" * 60)\n",
    "print(f\"\\nSentence 1: {ATTENTION_SENTENCES['coref_animal']!r}\")\n",
    "print(\"  Hypothesis: 'it' refers to 'animal' (because it was too tired)\")\n",
    "print(\"  Attention FROM each token TO 'animal' (last layer, avg over heads):\")\n",
    "for tok, attn in it_to_animal.items():\n",
    "    bar = \"█\" * int(attn * 50)\n",
    "    marker = \" ◀\" if tok == \"it\" else \"\"\n",
    "    print(f\"    {tok:12s} {bar} {attn:.4f}{marker}\")\n",
    "\n",
    "print(f\"\\nSentence 2: {ATTENTION_SENTENCES['coref_street']!r}\")\n",
    "print(\"  Hypothesis: 'it' refers to 'street' (because it was too wide)\")\n",
    "print(\"  Attention FROM each token TO 'street' (last layer, avg over heads):\")\n",
    "for tok, attn in it_to_street.items():\n",
    "    bar = \"█\" * int(attn * 50)\n",
    "    marker = \" ◀\" if tok == \"it\" else \"\"\n",
    "    print(f\"    {tok:12s} {bar} {attn:.4f}{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.4: GPT-2 attention (causal) vs BERT (bidirectional) ---\n",
    "test_sentence = ATTENTION_SENTENCES[\"agreement_short\"]\n",
    "print(f\"Sentence: {test_sentence!r}\\n\")\n",
    "\n",
    "bert_attn = extract_attention_weights(bert_model, test_sentence, DEVICE)\n",
    "gpt_attn = extract_attention_weights(\"gpt2\", test_sentence, DEVICE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# BERT — bidirectional (full matrix)\n",
    "ax = axes[0]\n",
    "attn = bert_attn[\"attentions\"][0, 0]\n",
    "sns.heatmap(\n",
    "    attn,\n",
    "    xticklabels=bert_attn[\"tokens\"],\n",
    "    yticklabels=bert_attn[\"tokens\"],\n",
    "    cmap=\"YlOrRd\",\n",
    "    vmin=0,\n",
    "    ax=ax,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    ")\n",
    "ax.set_title(\"BERT (Layer 0, Head 0) — Bidirectional\", fontsize=11)\n",
    "ax.set_xlabel(\"Key\")\n",
    "ax.set_ylabel(\"Query\")\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# GPT-2 — causal (lower triangular)\n",
    "ax = axes[1]\n",
    "attn = gpt_attn[\"attentions\"][0, 0]\n",
    "sns.heatmap(\n",
    "    attn,\n",
    "    xticklabels=gpt_attn[\"tokens\"],\n",
    "    yticklabels=gpt_attn[\"tokens\"],\n",
    "    cmap=\"YlOrRd\",\n",
    "    vmin=0,\n",
    "    ax=ax,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    ")\n",
    "ax.set_title(\"GPT-2 (Layer 0, Head 0) — Causal\", fontsize=11)\n",
    "ax.set_xlabel(\"Key\")\n",
    "ax.set_ylabel(\"Query\")\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Bidirectional vs Causal Attention — Real Attention Weights\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/figures/p5_bert_vs_gpt2.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Verify causal pattern numerically\n",
    "gpt_attn_matrix = gpt_attn[\"attentions\"][0, 0]\n",
    "upper_sum = float(np.triu(gpt_attn_matrix, k=1).sum())\n",
    "print(f\"\\nGPT-2 upper triangle sum: {upper_sum:.8f} (should be ~0 for causal)\")\n",
    "print(f\"Causal pattern verified: {upper_sum < 1e-6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Probe 6 — Architecture Comparison: Encoder vs Decoder vs Encoder-Decoder\n",
    "\n",
    "**Module**: `p6_arch_comparison.py`  \n",
    "**Models**: BERT-base, GPT-2, T5-small  \n",
    "\n",
    "From the course:\n",
    "> *\"Encoder-only models: Good for tasks that require understanding of the input\"*  \n",
    "> *\"Decoder-only models: Good for generative tasks such as text generation\"*  \n",
    "> *\"Encoder-decoder models: Good for generative tasks that require an input\"*\n",
    "\n",
    "We run all three architecture types on the **same input** and compare their\n",
    "hidden representations, output formats, and task-specific behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architecture_deepdive.probes.p6_arch_comparison import (\n",
    "    SHARED_INPUT,\n",
    "    probe_decoder_only,\n",
    "    probe_encoder_decoder,\n",
    "    probe_encoder_only,\n",
    ")\n",
    "\n",
    "print(f\"Shared input: {SHARED_INPUT!r}\\n\")\n",
    "\n",
    "# --- 6.1: Encoder-only (BERT) ---\n",
    "print(\"Running encoder-only probe (BERT)...\")\n",
    "enc_result = probe_encoder_only(device=DEVICE)\n",
    "show(\n",
    "    \"Encoder-Only: BERT\",\n",
    "    {\n",
    "        \"model\": enc_result[\"model\"],\n",
    "        \"output_type\": enc_result[\"output_type\"],\n",
    "        \"hidden_state_shape\": enc_result[\"hidden_state_shape\"],\n",
    "        \"tokens\": enc_result[\"tokens\"],\n",
    "        \"attention_is_bidirectional\": enc_result[\"attention_is_bidirectional\"],\n",
    "        \"cls_embedding_norm\": enc_result[\"cls_embedding_norm\"],\n",
    "        \"typical_use\": enc_result[\"typical_use\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.2: Decoder-only (GPT-2) ---\n",
    "print(\"Running decoder-only probe (GPT-2)...\")\n",
    "dec_result = probe_decoder_only(device=DEVICE)\n",
    "show(\n",
    "    \"Decoder-Only: GPT-2\",\n",
    "    {\n",
    "        \"model\": dec_result[\"model\"],\n",
    "        \"output_type\": dec_result[\"output_type\"],\n",
    "        \"logits_shape\": dec_result[\"logits_shape\"],\n",
    "        \"vocab_size\": dec_result[\"vocab_size\"],\n",
    "        \"attention_is_causal\": dec_result[\"attention_is_causal\"],\n",
    "        \"next_token_predictions\": dec_result[\"next_token_predictions\"],\n",
    "        \"typical_use\": dec_result[\"typical_use\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.3: Encoder-decoder (T5) ---\n",
    "print(\"Running encoder-decoder probe (T5)...\")\n",
    "encdec_result = probe_encoder_decoder(device=DEVICE)\n",
    "show(\n",
    "    \"Encoder-Decoder: T5\",\n",
    "    {\n",
    "        \"model\": encdec_result[\"model\"],\n",
    "        \"output_type\": encdec_result[\"output_type\"],\n",
    "        \"encoder_hidden_shape\": encdec_result[\"encoder_hidden_shape\"],\n",
    "        \"generated_text\": encdec_result[\"generated_text\"],\n",
    "        \"key_feature\": encdec_result[\"key_feature\"],\n",
    "        \"typical_use\": encdec_result[\"typical_use\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.4: Synthesis comparison table ---\n",
    "print(\"\\n\" + \"═\" * 80)\n",
    "print(\"  ARCHITECTURE COMPARISON SYNTHESIS\")\n",
    "print(\"═\" * 80)\n",
    "\n",
    "print(f\"\\n{'':30} {'Encoder-only':>16} {'Decoder-only':>16} {'Encoder-Decoder':>16}\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "comparison_rows = [\n",
    "    (\"Model\", enc_result[\"model\"], dec_result[\"model\"], encdec_result[\"model\"]),\n",
    "    (\"Attention\", \"Bidirectional\", \"Causal (L→R)\", \"Bidir + Causal\"),\n",
    "    (\"Output\", \"Embeddings\", \"Logits\", \"Generated seq\"),\n",
    "    (\"Best for\", \"Understanding\", \"Generation\", \"Seq2Seq\"),\n",
    "]\n",
    "\n",
    "for label, v1, v2, v3 in comparison_rows:\n",
    "    print(f\"{label:<30} {v1:>16} {v2:>16} {v3:>16}\")\n",
    "\n",
    "print('\\n\"Each of these parts can be used independently, depending on the task.\" — Course')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Decoder-only dominates** post-2022: 8 of 11 models in the course timeline are decoder-only,\n",
    "   reflecting the industry shift toward autoregressive LLMs.\n",
    "\n",
    "2. **CLM vs MLM context**: BERT (MLM) uses bidirectional context and predicts \"Paris\" with high\n",
    "   confidence using both left and right context. GPT-2 (CLM) can only use left context.\n",
    "\n",
    "3. **Transfer learning works**: Pretrained models achieve significantly higher accuracy on small\n",
    "   datasets, validating the course's claim that fine-tuning \"takes advantage of knowledge acquired\n",
    "   during pretraining.\"\n",
    "\n",
    "4. **Architecture anatomy**: BERT ~110M, GPT-2 ~124M, T5-small ~60M parameters. Despite similar\n",
    "   sizes, they have fundamentally different structures (encoder-only vs decoder-only vs both).\n",
    "\n",
    "5. **Attention patterns are verifiable**: BERT's attention matrix is full (bidirectional),\n",
    "   GPT-2's is lower-triangular (causal). The coreference test shows BERT can link \"it\" to its\n",
    "   correct antecedent using both left and right context.\n",
    "\n",
    "6. **Same input, different outputs**: Given the same text, the encoder produces embeddings,\n",
    "   the decoder produces next-token logits, and the encoder-decoder generates a new sequence.\n",
    "\n",
    "---\n",
    "*Reference*: [HuggingFace LLM Course — Chapter 1.4](https://huggingface.co/learn/llm-course/chapter1/4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}